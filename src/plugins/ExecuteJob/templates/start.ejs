// A wrapper for the torch script which:
//   - merges stdout, stderr
//   - receives some commands and uploads intermediate data
var spawn = require('child_process').spawn,
    fs = require('fs'),
    path = require('path'),
    log = console.error,
    logger = {};

// Create the stderr only logger
['error', 'warn', 'info', 'log', 'debug'].forEach(method => logger[method] = log);

// Get the BlobClient...
var COMMAND_PREFIX = '<%= START_CMD %>',
    IMAGE = '<%= IMAGE %>',
    requirejs = require('webgme').requirejs,
    remainingImageCount = 0,
    exitCode = null;

requirejs([
    'q',
    'blob/BlobClient'
], function(
    Q,
    BlobClient
) {
    var url = process.env.ORIGIN_URL || 'http://127.0.0.1:8888',
        CACHE_DIR = process.env.DEEPFORGE_CACHE || './worker-cache',
        protocol = url.split('://').shift(),
        address,
        port = (url.split(':') || ['80']).pop();

    address = url.replace(protocol + '://', '')
        .replace(':' + port, '');

    // Create CACHE_DIR if it doesn't exist
    var prepareCache = function() {
        return makeIfNeeded(CACHE_DIR);
    };

    var makeIfNeeded = function(dir) {
        var deferred = Q.defer(),
            job;

        log(`makeIfNeeded: ${JSON.stringify(dir)}`);
        fs.lstat(dir, (err, stat) => {

            if (err || !stat.isDirectory()) {
                fs.mkdir(dir, err => {
                    if (err) {
                        return deferred.reject(err);
                    }
                    deferred.resolve();
                });
            } else {
                deferred.resolve();
            }
        });
        return deferred.promise;
    };

    var blobClient = new BlobClient({
        server: address,
        httpsecure: protocol === 'https',
        serverPort: port,
        logger: logger
    });

    var checkFinished = () => {
        if (exitCode !== null && remainingImageCount === 0) {
            log('finished!');
            process.exit(exitCode);
        }
    };

    var uploadImage = function(line) {
        var args = line.split(/\s+/),
            name = args.slice(2).join(' ').replace(/\s+$/, ''),
            filename = 'metadata/' + name + '.png';

        // Upload the image from metadata/
        remainingImageCount++;
        fs.readFile(filename, (err, content) => {
            if (err) {
                logger.error(`Could not read ${filename}: ${err}`);
                return;
            }

            // Add hash to the image command
            log('about to putFile', filename);
            blobClient.putFile(filename, content)
                .then(hash => {
                    args.splice(2, 0, hash);
                    console.log(args.join(' '));
                    log('printing cmd:', args.join(' '));
                    --remainingImageCount;
                    log('finished uploading ' + filename + ' ' + remainingImageCount + ' remain');
                    checkFinished();
                })
                .fail(err => logger.error(`${filename} upload failed: ${err}`));
        });
    };

    var onStdout = function(data) {
        var lines = data.toString().split('\n'),
            result = [],
            cmdStart;

        // Check for commands...
        for (var i = 0; i < lines.length; i++) {
            cmdStart = lines[i].indexOf(COMMAND_PREFIX);
            if (cmdStart !== -1 && lines[i].indexOf(IMAGE) !== -1) {
                uploadImage(lines[i]);
            } else {
                result.push(lines[i]);
            }
        }

        process.stdout.write(result.join('\n'));
    };

    var createCacheDir = function(hash) {
        var dir = hash.substring(0, 2);
        return makeIfNeeded(CACHE_DIR + '/' + dir);
    };

    var dataCachePath = function(hash) {
        var dir = hash.substring(0, 2),
            filename = hash.substring(2),
            cachePath = `${CACHE_DIR}/${dir}/${filename}`;

        // Get the path for data in the cache
        return cachePath;
    };

    var makeSymLink = function(target, src) {
        var deferred = Q.defer(),
            job;

        src = path.resolve(src);
        target = path.resolve(target);
        fs.stat(src, err => {
            if (err.code === 'ENOENT') {
            logger.debug(`creating symlink "ln -s ${target} ${src}"`);
                job = spawn('ln', ['-s', target, src || '.']);
                job.on('exit', code => {
                    if (code) {
                        deferred.reject(`Could not create symlink ${target} -> ${src||'.'}`);
                        return;
                    }
                    deferred.resolve();
                });
            }
            deferred.resolve();
        });
        return deferred.promise;
    };

    var getData = function(ipath, hashes) {
        // Download the data and put it in the given path
        var deferred = Q.defer(),
            inputName = ipath.split('/')[1],
            cachePath = dataCachePath(hashes.cache);

        
        logger.debug(`retrieving ${ipath}`);
        fs.lstat(cachePath, (err, cacheStats) => {
            // Check if the data exists in the cache
            if (!err && cacheStats.isFile()) {
            logger.info(`${inputName} already cached. Skipping retrieval from blob`);
                return makeSymLink(cachePath, ipath).then(deferred.resolve);
            }

            createCacheDir(hashes.cache)
                .then(() => blobClient.getObject(hashes.req))
                .then(buffer => fs.writeFile(cachePath, buffer, (err, result) => {
                    if (err) {
                        logger.error('Retrieving ' + ipath + ' failed!');
                        return deferred.reject(`Could not write to ${ipath}: ${err}`);
                    }
                    // Create the symlink
                    logger.info('Retrieved ' + ipath);
                    return makeSymLink(cachePath, ipath).then(deferred.resolve);
                }))
                .fail(err => deferred.reject(`Could not retrieve "${inputName}" (${err})`));
        });

        return deferred.promise;
    };

    // Download the large files
    var inputData = JSON.parse(fs.readFileSync('./input-data.json')),
        inputPaths = Object.keys(inputData);

    // Request the data from the blob
    prepareCache()
        .then(() => Q.all(inputPaths.map(ipath => getData(ipath, inputData[ipath]))))
        .then(() => {
            // Run 'th init.lua' and merge the stdout, stderr
            var job = spawn('th', ['init.lua']);
            job.stdout.on('data', onStdout);
            job.stderr.on('data', data => process.stdout.write(data));
            job.on('close', code => {
                exitCode = code;
                log('script finished w/ exit code:', code);
                checkFinished();
            });
        })
        .fail(err => {
            console.log(`Data retrieval failed: ${err}`);
            process.exit(1);
        });
});
